{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA DEFINTTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kolom pada dataset**\n",
    "\n",
    "---\n",
    "<BR><BR>1. trans_date_trans_time:<BR>Waktu transaksi. Dapat digunakan untuk mendeteksi pola waktu (misal transaksi yang sering terjadi pada waktu yang tidak wajar bisa dicurigai sebagai fraud). \n",
    "<BR><BR>2. cc_num:<BR>Nomor kartu kredit. Informasi penting terkait pengguna kartu, tetapi tidak langsung membantu mendeteksi fraud kecuali ada pola dari penggunaan kartu yang tidak biasa.\n",
    "<BR><BR>3. merchant:<BR>Nama merchant (penjual). Merchant tertentu mungkin lebih sering terlibat dalam transaksi fraud, sehingga bisa membantu mendeteksi risiko terkait merchant.\n",
    "<BR><BR>4. category:<BR>Kategori transaksi. Fraud mungkin lebih sering terjadi dalam kategori tertentu seperti barang mewah atau hiburan, yang memiliki nilai transaksi lebih tinggi.\n",
    "<BR><BR>5. amt:<BR>Jumlah uang dalam transaksi. Nilai transaksi yang sangat besar atau tidak sesuai dengan pola belanja biasanya bisa menjadi tanda fraud.\n",
    "<BR><BR>6. first:<BR>Nama depan pemegang kartu. Tidak terlalu relevan dalam mendeteksi fraud secara langsung.\n",
    "<BR><BR>7. last:<BR>Nama belakang pemegang kartu. Sama seperti first, tidak relevan secara langsung.\n",
    "<BR><BR>8. gender:<BR>Jenis kelamin pemegang kartu. Bisa digunakan untuk melihat pola demografis terkait fraud, meskipun tidak secara langsung mengindikasikan fraud.\n",
    "<BR><BR>9. street:<BR>Alamat jalan pemegang kartu. Dapat digunakan dalam deteksi anomali jika lokasi transaksi berbeda jauh dari alamat pemegang kartu.\n",
    "<BR><BR>10. city:<BR>Kota pemegang kartu. Sama seperti street, dapat digunakan untuk memeriksa ketidaksesuaian antara lokasi pemegang kartu dan transaksi.\n",
    "<BR><BR>11. state:<BR>Negara bagian pemegang kartu. Sama dengan city, bisa mendeteksi anomali lokasi.\n",
    "<BR><BR>12. zip:<BR>Kode pos pemegang kartu. Sama dengan city dan state, bisa membantu mendeteksi anomali geografis.\n",
    "<BR><BR>13. lat:<BR>Garis lintang lokasi pemegang kartu. Lokasi geografis dapat membantu mendeteksi ketidaksesuaian jika dibandingkan dengan lokasi transaksi.\n",
    "<BR><BR>14. long:<BR>Garis bujur lokasi pemegang kartu. Sama dengan lat, membantu mendeteksi lokasi.\n",
    "<BR><BR>15. city_pop:<BR>Populasi kota pemegang kartu. Bisa digunakan untuk memahami risiko terkait daerah, misalnya daerah padat penduduk mungkin memiliki lebih banyak transaksi dan risiko.\n",
    "<BR><BR>16. job:<BR>Pekerjaan pemegang kartu. Pekerjaan dengan penghasilan tinggi mungkin lebih rentan terhadap fraud karena lebih sering terlibat dalam transaksi besar.\n",
    "<BR><BR>17. dob:<BR>Tanggal lahir pemegang kartu. Usia pemegang kartu bisa menjadi faktor, misalnya kelompok usia tertentu mungkin lebih rentan terhadap fraud.\n",
    "<BR><BR>18. trans_num:<BR>ID unik untuk transaksi. Tidak relevan dalam deteksi fraud secara langsung.\n",
    "<BR><BR>19. unix_time:<BR>Waktu transaksi dalam format unix. Sama seperti trans_date_trans_time, membantu dalam menganalisis pola waktu.\n",
    "<BR><BR>20. merch_lat:<BR>Garis lintang merchant. Dapat digunakan untuk mendeteksi ketidaksesuaian antara lokasi merchant dan pemegang kartu.\n",
    "<BR><BR>21. merch_long:<BR>Garis bujur merchant. Sama seperti merch_lat, membantu mendeteksi anomali lokasi.\n",
    "<BR><BR>22. is_fraud:<BR>Label apakah transaksi adalah fraud (0 = tidak fraud, 1 = fraud). Ini adalah target yang perlu diprediksi.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "#for Splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#for handling imbalance data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#for modelling\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "    # from sklearn.tree import DecisionTreeClassifier #ganti pakai random forest aja ah\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#for model evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "#for visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train = pd.read_csv('./archive/fraudTrain.csv')\n",
    "\n",
    "#oh iya baris di bawah ini nanti dimatikan saja\n",
    "#karena masih coba-coba codingnya dan di ulang, pakai 1000 baris dulu biar cepet aja\n",
    "#ntar kalau mau mulai pemodelan benerannya baru  pakai full data\n",
    "# df_train.sample(1000).to_csv('df_train_sampling.csv', index=False) #buang sekidikit ke csv untuk analisa ringan\n",
    "# df_train=df_train.sample(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## untuk save dan load pickle\n",
    "\n",
    "import pickle\n",
    "\n",
    "def save_model_to_pickle(model, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "    print(f\"Model saved to {filename}\")\n",
    "\n",
    "def load_model_from_pickle(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "    print(f\"Model loaded from {filename}\")\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA + Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# miniEDAtoPDF  nyomot dari internet\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, PageBreak\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "\n",
    "def mini_eda(df, pdf_filename):\n",
    "    # Create a PDF file\n",
    "    pdf = SimpleDocTemplate(pdf_filename, pagesize=letter)\n",
    "    styles = getSampleStyleSheet()\n",
    "    story = []\n",
    "\n",
    "    # Function to format numbers\n",
    "    def format_number(num):\n",
    "        return f\"{num:,.2f}\"\n",
    "\n",
    "    # Iterate through each column in the dataframe\n",
    "    for col in df.columns:\n",
    "        # Count of non-null rows\n",
    "        non_null_count = df[col].count()\n",
    "        story.append(Paragraph(f\"<b>Column: {col}</b>\", styles['Heading2']))\n",
    "        story.append(Paragraph(f\"Non-Null Rows: {format_number(non_null_count)}\", styles['BodyText']))\n",
    "\n",
    "        # Missing values\n",
    "        missing_count = df[col].isnull().sum()\n",
    "        missing_pct = (missing_count / len(df)) * 100\n",
    "        story.append(Paragraph(f\"Missing Values: {format_number(missing_count)} ({format_number(missing_pct)}%)\", styles['BodyText']))\n",
    "\n",
    "        # Distinct values\n",
    "        distinct_count = df[col].nunique()\n",
    "        distinct_pct = (distinct_count / len(df)) * 100\n",
    "        story.append(Paragraph(f\"Distinct Values: {format_number(distinct_count)} ({format_number(distinct_pct)}%)\", styles['BodyText']))\n",
    "\n",
    "        # Check if the column is numerical or categorical\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            # Numerical column\n",
    "            min_val = df[col].min()\n",
    "            max_val = df[col].max()\n",
    "            mean_val = df[col].mean()\n",
    "            q5 = df[col].quantile(0.05)\n",
    "            q25 = df[col].quantile(0.25)\n",
    "            q50 = df[col].quantile(0.50)\n",
    "            q75 = df[col].quantile(0.75)\n",
    "            q95 = df[col].quantile(0.95)\n",
    "            story.append(Paragraph(f\"Min: {format_number(min_val)}\", styles['BodyText']))\n",
    "            story.append(Paragraph(f\"Max: {format_number(max_val)}\", styles['BodyText']))\n",
    "            story.append(Paragraph(f\"Mean: {format_number(mean_val)}\", styles['BodyText']))\n",
    "            story.append(Paragraph(f\"5th Percentile (Q5): {format_number(q5)}\", styles['BodyText']))\n",
    "            story.append(Paragraph(f\"25th Percentile (Q25): {format_number(q25)}\", styles['BodyText']))\n",
    "            story.append(Paragraph(f\"50th Percentile (Q50): {format_number(q50)}\", styles['BodyText']))\n",
    "            story.append(Paragraph(f\"75th Percentile (Q75): {format_number(q75)}\", styles['BodyText']))\n",
    "            story.append(Paragraph(f\"95th Percentile (Q95): {format_number(q95)}\", styles['BodyText']))\n",
    "\n",
    "            # Check if the column can be divided into less than 20 groups\n",
    "            if distinct_count <= 20:\n",
    "                # Plot distribution\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                df[col].value_counts().sort_index().plot(kind='bar')\n",
    "                plt.title(f\"Distribution of {col}\")\n",
    "                plt.xlabel(col)\n",
    "                plt.ylabel(\"Count\")\n",
    "                img_filename = f\"{col}_distribution.png\"\n",
    "                plt.savefig(img_filename)\n",
    "                plt.close()\n",
    "                story.append(Spacer(1, 12))\n",
    "                story.append(Image(img_filename, width=400, height=300))\n",
    "            else:\n",
    "                story.append(Paragraph(f\"Min: {format_number(min_val)}\", styles['BodyText']))\n",
    "                story.append(Paragraph(f\"Max: {format_number(max_val)}\", styles['BodyText']))\n",
    "        else:\n",
    "            # Categorical column\n",
    "            story.append(Paragraph(f\"Distinct Values: {format_number(distinct_count)}\", styles['BodyText']))\n",
    "\n",
    "            # Top 10 + others\n",
    "            top_10 = df[col].value_counts().nlargest(10)\n",
    "            others_count = len(df) - top_10.sum()\n",
    "            others_pct = (others_count / len(df)) * 100\n",
    "            story.append(Paragraph(\"Top 10 + Others:\", styles['BodyText']))\n",
    "            for value, count in top_10.items():\n",
    "                pct = (count / len(df)) * 100\n",
    "                story.append(Paragraph(f\"{value}: {format_number(count)} ({format_number(pct)}%)\", styles['BodyText']))\n",
    "            story.append(Paragraph(f\"Others: {format_number(others_count)} ({format_number(others_pct)}%)\", styles['BodyText']))\n",
    "\n",
    "            # Plot distribution\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            top_10.plot(kind='bar')\n",
    "            plt.title(f\"Top 10 Distribution of {col}\")\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel(\"Count\")\n",
    "            img_filename = f\"{col}_top10_distribution.png\"\n",
    "            plt.savefig(img_filename)\n",
    "            plt.close()\n",
    "            story.append(Spacer(1, 12))\n",
    "            story.append(Image(img_filename, width=400, height=300))\n",
    "\n",
    "        # Add a page break after each column\n",
    "        story.append(PageBreak())\n",
    "\n",
    "    # Build the PDF\n",
    "    pdf.build(story)\n",
    "\n",
    "# Example usage\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "# mini_eda(df, 'eda_report.pdf')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_eda(df_train, 'eda_report__df_train.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tukang stop proses jupyter , uncomment to break the running process of \"Run All\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert date and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert kolom 'trans_date_trans_time' ke format datetime\n",
    "df_train['trans_date_trans_time'] = pd.to_datetime(df_train['trans_date_trans_time'])\n",
    "\n",
    "# Ambil komponen waktu dari kolom 'trans_date_trans_time'\n",
    "df_train['trans_year'] = df_train['trans_date_trans_time'].dt.year\n",
    "df_train['trans_month'] = df_train['trans_date_trans_time'].dt.month\n",
    "df_train['trans_date'] = df_train['trans_date_trans_time'].dt.day\n",
    "df_train['trans_hour'] = df_train['trans_date_trans_time'].dt.hour\n",
    "df_train['trans_dow'] = df_train['trans_date_trans_time'].dt.dayofweek  # Hari dalam minggu (0 = Senin, 6 = Minggu)\n",
    "\n",
    "# Tampilkan beberapa baris pertama buat ngecek hasilnya\n",
    "df_train.sample(15)[['trans_date_trans_time', 'trans_year', 'trans_month', 'trans_date', 'trans_hour', 'trans_dow']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert kolom 'unix_time' ke format datetime\n",
    "df_train['unix_time'] = pd.to_datetime(df_train['unix_time'], unit='s')\n",
    "\n",
    "# Tambahin 7 tahun ke 'unix_time'\n",
    "seconds_in_7_years = 7 * 365 * 24 * 60 * 60  # Hitung detik dalam 7 tahun\n",
    "df_train['unix_time'] = df_train['unix_time'] + pd.Timedelta(seconds=seconds_in_7_years)\n",
    "\n",
    "# Ambil komponen waktu dari kolom 'unix_time'\n",
    "df_train['unix_year'] = df_train['unix_time'].dt.year+7\n",
    "df_train['unix_month'] = df_train['unix_time'].dt.month\n",
    "df_train['unix_date'] = df_train['unix_time'].dt.day\n",
    "df_train['unix_hour'] = df_train['unix_time'].dt.hour\n",
    "df_train['unix_dow'] = df_train['unix_time'].dt.dayofweek  # Hari dalam minggu (0 = Senin, 6 = Minggu)\n",
    "\n",
    "# Tampilkan beberapa baris pertama buat ngecek hasilnya\n",
    "df_train.sample(10)[['unix_time', 'unix_year', 'unix_month', 'unix_date', 'unix_hour', 'unix_dow']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membandingkan trans time dan unix time\n",
    "df_train.sample(15)[['trans_date_trans_time', 'trans_year', 'trans_month', 'trans_date', 'trans_hour', 'trans_dow','unix_time', 'unix_year', 'unix_month', 'unix_date', 'unix_hour', 'unix_dow']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## menghitung selisih hari saat transaksi dan pembukuan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat kolom 'dow_dif' selisih hari transaksi dengan pembukan\n",
    "df_train['dow_dif'] = df_train.apply(\n",
    "    lambda row: row['trans_dow'] - row['unix_dow'] \n",
    "    if row['trans_dow'] - row['unix_dow'] >= 0 \n",
    "    else row['trans_dow'] - row['unix_dow'] + 7, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tampilkan hasilnya\n",
    "df_train.sample(30)[['unix_dow', 'trans_dow', 'dow_dif']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## menghitung jarak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menghitung jarak menggunakan rumus Haversine (minta sama chatgpt)\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Konversi dari derajat ke radian\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # Rumus Haversine\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    \n",
    "    # Radius bumi dalam kilometer (bisa diganti dengan 6371 untuk km atau 3958.8 untuk miles)\n",
    "    R = 6371  # Radius bumi dalam kilometer\n",
    "    distance = R * c\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat kolom 'card_merchant_distance' dengan menghitung jarak\n",
    "df_train['card_merchant_distance_km'] = df_train.apply(\n",
    "    lambda row: haversine(row['lat'], row['long'], row['merch_lat'], row['merch_long']), axis=1\n",
    ")\n",
    "\n",
    "#membuat grouping jarak per 10 km, akan digunakan untuk  membuat kelompok fraud rate\n",
    "df_train['card_merchant_distance_km_grp']=(df_train['card_merchant_distance_km']//10)*10\n",
    "\n",
    "# Tampilkan hasil\n",
    "df_train.sample(10)[['lat', 'long', 'merch_lat', 'merch_long', 'card_merchant_distance_km','card_merchant_distance_km_grp']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## menghitung umur saat transaksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'dob' ke datetime format supaya bisa diproses\n",
    "df_train['dob'] = pd.to_datetime(df_train['dob'])\n",
    "\n",
    "# Hitung umur berdasarkan selisih antara 'unix_year' dan tahun dari 'dob'\n",
    "df_train['person_age'] = df_train['unix_year'] - df_train['dob'].dt.year\n",
    "\n",
    "#membuat grouping umu per 10 tahun, akan digunakan untuk  membuat kelompok fraud rate\n",
    "df_train['person_age_grp']=(df_train['person_age']//10)*10\n",
    "\n",
    "# Tampilkan hasilnya\n",
    "df_train.sample(10)[['dob', 'unix_year', 'person_age','person_age_grp']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fraud rate by unix_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setiap jam di group, lalu dihitung jumlah transaksinya (jml barisnya aka size nya) \n",
    "# dan jumlah transaksi fraud nya (is_fraud==1)--(di jumlah saja)\n",
    "hourly_data = df_train.groupby('unix_hour').agg(\n",
    "    total_transactions=('is_fraud', 'size'),\n",
    "    fraud_cases=('is_fraud', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "#hitung rate nya\n",
    "hourly_data['fraud_rate_by_unix_hour'] = hourly_data['fraud_cases'] / hourly_data['total_transactions']\n",
    "\n",
    "#masukkan kembali ke dataframe utama\n",
    "df_train = df_train.merge(hourly_data[['unix_hour', 'fraud_rate_by_unix_hour']], on='unix_hour', how='left')\n",
    "\n",
    "#melihat isi df_train yang baru secara random sebanyak 10 baris\n",
    "df_train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the fraud rate for each hour\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.bar(hourly_data['unix_hour'], hourly_data['fraud_rate_by_unix_hour'], color='skyblue', edgecolor='black')\n",
    "plt.title('Fraud Rate Distribution by Hour of the Day (is_fraud = 1)')\n",
    "plt.xlabel('Hour of the Day (unix_hour)')\n",
    "plt.ylabel('Fraud Rate')\n",
    "plt.xticks(range(0, 24))  # Display hours from 0 to 23\n",
    "plt.grid(True, axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fraud rate by category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# menerapkan cara kerja yang sama, dengan \"fraud rate by unix_hour\"\n",
    "# comment lebih lengkap ada di sana\n",
    "\n",
    "\n",
    "category_data = df_train.groupby('category').agg(\n",
    "    total_transactions=('is_fraud', 'size'),\n",
    "    fraud_cases=('is_fraud', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "category_data['fraud_rate_by_category'] = category_data['fraud_cases'] / category_data['total_transactions']\n",
    "\n",
    "df_train = df_train.merge(category_data[['category', 'fraud_rate_by_category']], on='category', how='left')\n",
    "\n",
    "df_train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the fraud rate for each hour\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.bar(category_data['category'], category_data['fraud_rate_by_category'], color='skyblue', edgecolor='black')\n",
    "plt.title('Fraud Rate Distribution by Hour of the Day (is_fraud = 1)')\n",
    "plt.xlabel('Hour of the Day (category)')\n",
    "plt.ylabel('Fraud Rate')\n",
    "plt.xticks(range(0, 24), rotation=90)  # Rotate x-axis labels by 90 degrees\n",
    "plt.grid(True, axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_categories = df_train['category'].unique()\n",
    "print(unique_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fraud rate by city\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_data = df_train.groupby('city').agg(\n",
    "    total_transactions=('city', 'count'),\n",
    "    fraud_cases=('is_fraud', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "city_data['fraud_rate_by_city'] = city_data['fraud_cases'] / city_data['total_transactions']\n",
    "\n",
    "city_data.sort_values(by=['total_transactions', 'fraud_rate_by_city'], ascending=[True, True])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memakai fraud rate untuk city , menyebabkan data jadi kurang tidak berimbang untuk kota kecil ataupun transaksi sedikit\n",
    "# akan dilakuan mmemasukkan komponen city population dalam  perhitungn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_pop_data = df_train.groupby(['city']).agg(\n",
    "    mean_pop=('city_pop','mean')\n",
    ").reset_index()\n",
    "city_pop_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(city_data.columns)\n",
    "print(city_pop_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#masukkan data populasi rata2 city ke city_data\n",
    "city_data  = city_data.merge(city_pop_data[['city','mean_pop']], on='city', how='left')\n",
    "\n",
    "#add scaling untuk populasi\n",
    "city_data['max_overal_pop']=city_data['mean_pop'].max()\n",
    "city_data['scaled_pop']=city_data['mean_pop'] / city_data['max_overal_pop']\n",
    "city_data['fraud_rate_by_city_scaled']=city_data['fraud_rate_by_city'] * city_data['scaled_pop']\n",
    "\n",
    "city_data.sort_values(by=['total_transactions', 'fraud_rate_by_city_scaled'], ascending=[True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.merge(city_data[['city', 'fraud_rate_by_city_scaled']], on='city', how='left')\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the top jmlnya states with the highest fraud rate\n",
    "jmlnya = 10\n",
    "top_rate_cities = df_train.groupby('city')['fraud_rate_by_city_scaled'].mean().sort_values(ascending=False).head(jmlnya)\n",
    "\n",
    "# Plot the fraud rate for the top jmlnya cities\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(top_rate_cities.index, top_rate_cities.values, color='skyblue', edgecolor='black')\n",
    "plt.title('Top 10 Cities with Highest Fraud Rate')\n",
    "plt.xlabel('City')\n",
    "plt.ylabel('Fraud Rate')\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels by 90 degrees\n",
    "plt.grid(True, axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tukang stop proses jupyter , uncomment to break the running process of \"Run All\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_stats = df_train.groupby('category')['amt'].describe(percentiles=[.05, .95])\n",
    "category_stats = category_stats.rename(columns={'5%': 'cat_amt_5pctl', '95%': 'cat_amt_95pctl'})\n",
    "category_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## flagging outlier untuk Category VS Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join ke df_train\n",
    "df_train= df_train.merge(category_stats[['cat_amt_5pctl','cat_amt_95pctl']], on='category', how = 'left')\n",
    "df_train['category_lower_outlier']=np.where(df_train['amt'] < df_train['cat_amt_5pctl'],1,0)\n",
    "df_train['category_upper_outlier']=np.where(df_train['amt'] > df_train['cat_amt_95pctl'],1,0)\n",
    "#drop kolom bantuan \n",
    "df_train=df_train.drop('cat_amt_5pctl', axis=1)\n",
    "df_train=df_train.drop('cat_amt_95pctl', axis=1)\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_stats = df_train.groupby('job')['amt'].describe(percentiles=[.05, .95])\n",
    "job_stats = job_stats.rename(columns={'5%': 'job_amt_5pctl', '95%': 'job_amt_95pctl'})\n",
    "job_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## flagging outlier untuk Job VS Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join ke df_train\n",
    "df_train= df_train.merge(job_stats[['job_amt_5pctl','job_amt_95pctl']], on='job', how = 'left')\n",
    "df_train['job_lower_outlier']=np.where(df_train['amt'] < df_train['job_amt_5pctl'],1,0)\n",
    "df_train['job_upper_outlier']=np.where(df_train['amt'] > df_train['job_amt_95pctl'],1,0)\n",
    "#drop kolom bantuan \n",
    "df_train=df_train.drop('job_amt_5pctl', axis=1)\n",
    "df_train=df_train.drop('job_amt_95pctl', axis=1)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO yang belon dikerjakan untuk EDA di atas:\n",
    "\n",
    "- apa perlu flagging highrisk juga semacam jarak vs isFraud == 1 ?  >>>>> DONE di bawah\n",
    "- apa perlu flagging highrisk juga semacam umur vs isFraud == 1 ?  >>>>> DONE di bawah\n",
    "- apa perlu flagging highrisk juga semacam trxtime_bookingtime_diff vs isFraud == 1 ?  >>>>> DONE di bawah\n",
    "\n",
    "- CC usage count ( kecil2 tapi banyak atau sekali tapi langsung big amount) ---- gimana cara modeling data / chart nya ? (mungkin bisa pakai cummulative trx count dan avarage amount per CC per bulan bersangkutan ?) >>>>> SKIP gak ada waktu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fraud rate by umur group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# menerapkan cara kerja yang sama, dengan \"fraud rate by unix_hour\"\n",
    "# comment lebih lengkap ada di sana\n",
    "\n",
    "umur_data = df_train.groupby('person_age_grp').agg(\n",
    "    total_transactions=('is_fraud', 'size'),\n",
    "    fraud_cases=('is_fraud', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "umur_data['fraud_rate_by_umur'] = umur_data['fraud_cases'] / umur_data['total_transactions']\n",
    "\n",
    "df_train = df_train.merge(umur_data[['person_age_grp', 'fraud_rate_by_umur']], on='person_age_grp', how='left')\n",
    "\n",
    "df_train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(umur_data['person_age_grp'], umur_data['fraud_rate_by_umur'], color='skyblue', edgecolor='black', alpha=0.7)\n",
    "\n",
    "plt.title('Fraud Rate by Umur Group', fontsize=16)\n",
    "plt.xlabel('Card Merchant Umur Group (person_age_grp)', fontsize=14)\n",
    "plt.ylabel('Fraud Rate by Umur', fontsize=14)\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fraud rate by jarak group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# menerapkan cara kerja yang sama, dengan \"fraud rate by unix_hour\"\n",
    "# comment lebih lengkap ada di sana\n",
    "\n",
    "distance_data = df_train.groupby('card_merchant_distance_km_grp').agg(\n",
    "    total_transactions=('is_fraud', 'size'),\n",
    "    fraud_cases=('is_fraud', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "distance_data['fraud_rate_by_distance'] = distance_data['fraud_cases'] / distance_data['total_transactions']\n",
    "\n",
    "df_train = df_train.merge(distance_data[['card_merchant_distance_km_grp', 'fraud_rate_by_distance']], on='card_merchant_distance_km_grp', how='left')\n",
    "\n",
    "df_train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(distance_data['card_merchant_distance_km_grp'], distance_data['fraud_rate_by_distance'], color='skyblue', edgecolor='black', alpha=0.7)\n",
    "\n",
    "plt.title('Fraud Rate by Distance Group', fontsize=16)\n",
    "plt.xlabel('Card Merchant Distance Group (card_merchant_distance_km_grp)', fontsize=14)\n",
    "plt.ylabel('Fraud Rate by Distance', fontsize=14)\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fraud rate by dow_diff (selisih transaksi vs  pembukuan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# menerapkan cara kerja yang sama, dengan \"fraud rate by unix_hour\"\n",
    "# comment lebih lengkap ada di sana\n",
    "\n",
    "day_diff_data = df_train.groupby('dow_dif').agg(\n",
    "    total_transactions=('is_fraud', 'size'),\n",
    "    fraud_cases=('is_fraud', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "day_diff_data['fraud_rate_by_dow_diff'] = day_diff_data['fraud_cases'] / day_diff_data['total_transactions']\n",
    "\n",
    "df_train = df_train.merge(day_diff_data[['dow_dif', 'fraud_rate_by_dow_diff']], on='dow_dif', how='left')\n",
    "\n",
    "df_train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(day_diff_data['dow_dif'], day_diff_data['fraud_rate_by_dow_diff'], color='skyblue', edgecolor='black', alpha=0.7)\n",
    "\n",
    "plt.title('Fraud Rate by DoW_Diff Group', fontsize=16)\n",
    "plt.xlabel('Card Merchant DoW_Diff Group (dow_dif)', fontsize=14)\n",
    "plt.ylabel('Fraud Rate by DoW_Diff', fontsize=14)\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dummy var untuk Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train = df_train.copy(deep=True)\n",
    "\n",
    "#KARENA SUDAH PAKAI FLAGGING 'risk_level_of_category' , tidak perlu dijadikan variable dummy (label encoding ataupun OHE)\n",
    "    # #copy data dulu untuk keperluan EDA (e.g. check missng value, etc)\n",
    "    # df_train['category_ori'] = df_train['category']\n",
    "    # #membuath dummy variable (OHE)\n",
    "    # df_train = pd.get_dummies(df_train, columns=['category'], drop_first=True)\n",
    "\n",
    "\n",
    "#copy data dulu untuk keperluan EDA (e.g. check missng value, etc)\n",
    "# df_train['gender_ori'] = df_train['gender']\n",
    "\n",
    "#membuath dummy variable (OHE)\n",
    "df_train = pd.get_dummies(df_train, columns=['gender'], drop_first=True)\n",
    "\n",
    "# df_train.sample(5)\n",
    "# df_train.sample(30).to_csv('df_train_encoded_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_eda(df_train, 'eda_report__df_train_withDummies.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tukang stop proses jupyter , uncomment to break the running process of \"Run All\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tukang stop proses jupyter , uncomment to break the running process of \"Run All\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split antara feature dan target \n",
    "\n",
    "X = df_train.drop('is_fraud', axis=1)\n",
    "y = df_train['is_fraud']\n",
    "\n",
    "\n",
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split training data into training and validation sets (80% train, 20% test)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# X_train dan y_train digabung lagi dulu karen akan dilakukan handling imbalance data \n",
    "df_train_8080 = pd.concat([X_train, y_train], axis=1)\n",
    "df_val_8020 = pd.concat([X_val, y_val], axis=1)\n",
    "df_test_20 = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# Optionally, check the shape of the split data\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Validation features shape: {X_val.shape}\")\n",
    "print(f\"Test features shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_distribution = df_train_8080['is_fraud'].value_counts()\n",
    "print(target_distribution)\n",
    "print('-------------------')\n",
    "print(target_distribution / target_distribution.sum() * 100)\n",
    "print('-------------------')\n",
    "\n",
    "fraud_cases = target_distribution[1]  \n",
    "non_fraud_cases = target_distribution[0]  \n",
    "imbalance_ratio = non_fraud_cases / fraud_cases\n",
    "print(f\"Imbalance ratio (Non-fraud / Fraud): {imbalance_ratio:.2f}\")\n",
    "print('-------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install dulu kalau belum ada di env kita\n",
    "#!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_8080.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = df_train_8080.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(20,20))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#memilih feature secara manual utuk feature2 yang \n",
    "\n",
    "list_of_selected_field=[\n",
    "# 'Unnamed:0',\n",
    "# 'trans_date_trans_time',\n",
    "# 'cc_num',\n",
    "# 'merchant',\n",
    "# 'category',\n",
    "'amt',\n",
    "# 'first',\n",
    "# 'last',\n",
    "# 'street',\n",
    "# 'city',\n",
    "# 'state',\n",
    "# 'zip',\n",
    "# 'lat',\n",
    "# 'long',\n",
    "'city_pop',\n",
    "# 'job',\n",
    "# 'dob',\n",
    "# 'trans_num',\n",
    "# 'unix_time',\n",
    "# 'merch_lat',\n",
    "# 'merch_long',\n",
    "'is_fraud',\n",
    "# 'trans_year',\n",
    "# 'trans_month',\n",
    "# 'trans_date',\n",
    "# 'trans_hour',\n",
    "# 'trans_dow',\n",
    "'unix_year',\n",
    "'unix_month',\n",
    "'unix_date',\n",
    "'unix_hour',\n",
    "'unix_dow',\n",
    "'dow_dif',\n",
    "# 'card_merchant_distance_km',\n",
    "'card_merchant_distance_km_grp',\n",
    "# 'person_age',\n",
    "'person_age_grp',\n",
    "'fraud_rate_by_unix_hour',\n",
    "'fraud_rate_by_category',\n",
    "'fraud_rate_by_city_scaled',\n",
    "'category_lower_outlier',\n",
    "'category_upper_outlier',\n",
    "'job_lower_outlier',\n",
    "'job_upper_outlier',\n",
    "'fraud_rate_by_umur',\n",
    "'fraud_rate_by_distance',\n",
    "'fraud_rate_by_dow_diff',\n",
    "'gender_M'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitted Data and Selected Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_8080_selected = df_train_8080[list_of_selected_field]\n",
    "df_val_8020_selected = df_val_8020[list_of_selected_field]\n",
    "df_test_20_selected= df_test_20[list_of_selected_field]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### melihat correlation map untuk data training yang featurenya sudah dipilih"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = df_train_8080_selected.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(20,20))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train_8080_selected.drop('is_fraud', axis=1)\n",
    "y = df_train_8080_selected['is_fraud']\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smoted, y_smoted = smote.fit_resample(X, y)\n",
    "\n",
    "df_train_balanced_smote = pd.DataFrame(X_smoted, columns=X.columns)\n",
    "df_train_balanced_smote['is_fraud'] = y_smoted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_distribution = df_train_balanced_smote['is_fraud'].value_counts()\n",
    "print(target_distribution)\n",
    "print('-------------------')\n",
    "print(target_distribution / target_distribution.sum() * 100)\n",
    "print('-------------------')\n",
    "\n",
    "fraud_cases = target_distribution[1]  \n",
    "non_fraud_cases = target_distribution[0]  \n",
    "imbalance_ratio = non_fraud_cases / fraud_cases\n",
    "print(f\"Imbalance ratio (Non-fraud / Fraud): {imbalance_ratio:.2f}\")\n",
    "print('-------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "====================================================================<br>\n",
    "disini data sudah di split dan sudah dipilih beberapa feature yang akan dipakai (manual)<br>\n",
    "tinggal memilih model yang ingin dipkai<br>\n",
    "====================================================================<br>\n",
    "data yang siap digunakan :<br>\n",
    "- df_train_balanced_smote : untuk data training\n",
    "- df_val_8020_selected  : untuk data validasi\n",
    "- df_test_20_selected : untuk data test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADABOOST MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Adaboost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "base_estimator = RandomForestClassifier() # tidak ada parameter di set , saya ambil default saja semua \n",
    "\n",
    "# Initialize AdaBoost with base estimator and n_estimators (number of trees)\n",
    "model = AdaBoostClassifier(\n",
    "        estimator=base_estimator, \n",
    "        n_estimators=100, \n",
    "        random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Akan Fitting Model, [AWAS LAMA!!!!!]\n",
    "\n",
    " skip saja kalau udah pernah , langsung load saja di bawah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train_balanced_smote.drop('is_fraud', axis=1)\n",
    "y = df_train_balanced_smote['is_fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/Load Model to/from File (bisa di skip kalau tidak perlu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save modelnya dulu \n",
    "nama_filenya = 'model_adaboost_RandomForestClassifier_n100.pkl'\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "\n",
    "nama_filenya = nama_filenya.replace('.pkl', f'_{timestamp}.pkl')\n",
    "save_model_to_pickle(model,nama_filenya)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load modelnya \n",
    "# nama_filenya = 'model_adaboost_RandomForestClassifier_n100_20250227_0613.pkl' #<<< ganti nama file nya sesuai dengan model yang ingin di load\n",
    "model = load_model_from_pickle(nama_filenya)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = model.feature_importances_\n",
    "\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns, \n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dari feature importance di atas, kita akan coba ambil lagi natni buat model ke 2 yang menggunakan feature2 yang importancenya 1% ke atas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = df_val_8020_selected.drop('is_fraud', axis=1)\n",
    "y_val = df_val_8020_selected['is_fraud']\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Predict probabilities for ROC-AUC evaluation\n",
    "y_pred_prob = model.predict_proba(X_val)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Predicted Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report (precision, recall, f1-score)\n",
    "print(\"Classification Report:\\n\", classification_report(y_val, y_pred))\n",
    "print('---------------------------------------------------')\n",
    "# Confusion matrix to understand true positives, false positives, etc.\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "cm_df = pd.DataFrame(cm, index=['Actual Non-Fraud', 'Actual Fraud'], columns=['Predicted Non-Fraud', 'Predicted Fraud'])\n",
    "print(\"Confusion Matrix:\\n\", cm_df )\n",
    "print('---------------------------------------------------')\n",
    "# Calculate ROC-AUC score for model's ability to distinguish between classes\n",
    "roc_auc = roc_auc_score(y_val, y_pred_prob)\n",
    "print(\"ROC-AUC Score:\", roc_auc)\n",
    "print('---------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test_20_selected.drop('is_fraud', axis=1)\n",
    "y_test = df_test_20_selected['is_fraud']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicted Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Predict probabilities for ROC-AUC evaluation \n",
    "y_pred_prob = model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Test Data Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report (precision, recall, f1-score)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print('---------------------------------------------------')\n",
    "# Confusion matrix to understand true positives, false positives, etc.\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_df = pd.DataFrame(cm, index=['Actual Non-Fraud', 'Actual Fraud'], columns=['Predicted Non-Fraud', 'Predicted Fraud'])\n",
    "print(\"Confusion Matrix:\\n\", cm_df )\n",
    "print('---------------------------------------------------')\n",
    "# Calculate ROC-AUC score for model's ability to distinguish between classes\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(\"ROC-AUC Score:\", roc_auc)\n",
    "print('---------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simpan report classification untuk pembandingan di bagian paling akhir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryAkhir = 'Classification Report untuk Test Data menggunakan model dengan feature pilihan manual\\n'\n",
    "summaryAkhir = summaryAkhir + classification_report(y_test, y_pred)\n",
    "summaryAkhir = summaryAkhir +'----------------------------------------------------------\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADABOOST MODELING (LAGI)\n",
    "(kali ini akan dicoba menggunakan feature yang kontribusinya lebih dari 1% saja berdasar feature importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection based on Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#memilih feature2 yang kontribusinya >= 1% saja\n",
    "list_of_selected_field2 = feature_importance_df[feature_importance_df['Importance']>=0.01]\n",
    "list_of_selected_field2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list feature feature pilihan berdasar feature importance\n",
    "importantFeatures = list(list_of_selected_field2['Feature'].unique()) \n",
    "importantFeatures_and_target = importantFeatures + ['is_fraud']\n",
    "\n",
    "df_val_8020_penting = df_val_8020_selected[importantFeatures_and_target]\n",
    "df_test_20_penting= df_test_20_selected[importantFeatures_and_target]\n",
    "df_train_balanced_smote_penting = df_train_balanced_smote[importantFeatures_and_target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "====================================================================<br>\n",
    "disini data sudah di split dan sudah dipilih beberapa feature berdasar feature importance<br>\n",
    "tinggal memilih model yang ingin dipkai<br>\n",
    "====================================================================<br>\n",
    "data yang siap digunakan :<br>\n",
    "- df_train_balanced_smote_penting : untuk data training\n",
    "- df_val_8020_penting  : untuk data validasi\n",
    "- df_test_20_penting : untuk data test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### awas lama looh di sini (about 15 minutes on pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train_balanced_smote_penting[importantFeatures]\n",
    "y = df_train_balanced_smote_penting['is_fraud']\n",
    "\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save / Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save modelnya dulu \n",
    "nama_filenya = 'model_adaboost_RandomForestClassifier_penting.pkl'\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "\n",
    "nama_filenya = nama_filenya.replace('.pkl', f'_{timestamp}.pkl')\n",
    "save_model_to_pickle(model,nama_filenya)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nama_filenya = 'model_adaboost_RandomForestClassifier_featured.pkl' #ganti dengan nama file lain yang dinginkan\n",
    "model = load_model_from_pickle(nama_filenya)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = df_val_8020_penting.drop('is_fraud', axis=1)\n",
    "y_val = df_val_8020_penting['is_fraud']\n",
    "\n",
    "## Predict on the test set\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "## Predict probabilities for ROC-AUC evaluation\n",
    "y_pred_prob = model.predict_proba(X_val)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Predicted Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'y_val values :\\n{y_val.value_counts()}')\n",
    "y_pred_series = pd.Series(y_pred)\n",
    "print(f'y_pred values :\\n{y_pred_series.value_counts()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report (precision, recall, f1-score)\n",
    "print(\"Classification Report:\\n\", classification_report(y_val, y_pred))\n",
    "print('---------------------------------------------------')\n",
    "# Confusion matrix to understand true positives, false positives, etc.\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "cm_df = pd.DataFrame(cm, index=['Actual Non-Fraud', 'Actual Fraud'], columns=['Predicted Non-Fraud', 'Predicted Fraud'])\n",
    "print(\"Confusion Matrix:\\n\", cm_df )\n",
    "print('---------------------------------------------------')\n",
    "# Calculate ROC-AUC score for model's ability to distinguish between classes\n",
    "roc_auc = roc_auc_score(y_val, y_pred_prob)\n",
    "print(\"ROC-AUC Score:\", roc_auc)\n",
    "print('---------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicted Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test_20_penting.drop('is_fraud', axis=1)\n",
    "y_test = df_test_20_penting['is_fraud']\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Predict probabilities for ROC-AUC evaluation \n",
    "y_pred_prob = model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Test Data Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report (precision, recall, f1-score)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print('---------------------------------------------------')\n",
    "# Confusion matrix to understand true positives, false positives, etc.\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_df = pd.DataFrame(cm, index=['Actual Non-Fraud', 'Actual Fraud'], columns=['Predicted Non-Fraud', 'Predicted Fraud'])\n",
    "print(\"Confusion Matrix:\\n\", cm_df )\n",
    "print('---------------------------------------------------')\n",
    "# Calculate ROC-AUC score for model's ability to distinguish between classes\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(\"ROC-AUC Score:\", roc_auc)\n",
    "print('---------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPARE RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryAkhir = summaryAkhir + 'Classification Report untuk Test Data menggunakan model dengan important feature saja\\n'\n",
    "summaryAkhir = summaryAkhir + classification_report(y_test, y_pred)\n",
    "summaryAkhir = summaryAkhir +'----------------------------------------------------------\\n'\n",
    "\n",
    "print(summaryAkhir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BARIS PALING BAWAH SAAT INI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (datly)",
   "language": "python",
   "name": "datly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
